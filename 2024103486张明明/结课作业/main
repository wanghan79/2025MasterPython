import ast
import os
import re
from collections import Counter

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_score, \
    recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import random
import pickle
import math
from typing import List, Tuple, Dict, Optional, Union
from torch.cuda.amp import GradScaler, autocast


def set_seed(seed: int = 42) -> None:
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


set_seed(42)

# 检查设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


# 数据增强策略定义
class AAPool:
    HYDROPHOBIC = ['A', 'I', 'L', 'M', 'F', 'W', 'V']
    HYDROPHILIC = ['R', 'D', 'N', 'E', 'Q', 'H', 'K', 'S', 'T']
    SPECIAL = ['C', 'G', 'P']
    ALL = HYDROPHOBIC + HYDROPHILIC + SPECIAL

    @staticmethod
    def get_similarity_groups() -> Dict[str, List[str]]:
        """获取氨基酸相似性分组（基于物理化学性质）"""
        return {
            'A': ['G', 'S'],
            'R': ['K', 'H'],
            'N': ['D', 'Q'],
            'D': ['E', 'N'],
            'C': ['S', 'T'],
            'Q': ['E', 'N'],
            'E': ['D', 'Q'],
            'G': ['A', 'S'],
            'H': ['R', 'K'],
            'I': ['L', 'V'],
            'L': ['I', 'V'],
            'K': ['R', 'H'],
            'M': ['L', 'I'],
            'F': ['Y', 'W'],
            'P': ['A', 'G'],
            'S': ['T', 'A'],
            'T': ['S', 'A'],
            'W': ['Y', 'F'],
            'Y': ['F', 'W'],
            'V': ['I', 'L'],
            'X': []  # 填充字符
        }

    @staticmethod
    def get_property_groups() -> Dict[str, List[str]]:
        """按物理化学性质分组氨基酸"""
        return {
            'hydrophobic': AAPool.HYDROPHOBIC,
            'hydrophilic': AAPool.HYDROPHILIC,
            'special': AAPool.SPECIAL
        }


# 数据加载和预处理
class EnhancedEnzymeDataset(Dataset):

    def __init__(self,
                 file_path: str,
                 window_size: int = 15,
                 augment: bool = True,
                 augment_strategy: str = "similarity",
                 use_positional_encoding: bool = True,
                 max_seq_length: int = 1000):

        self.df = pd.read_csv(file_path)
        self.sequences = self.df['aa_sequence'].tolist()
        self.augment = augment
        self.augment_strategy = augment_strategy
        self.window_size = window_size
        self.use_positional_encoding = use_positional_encoding
        self.max_seq_length = max_seq_length

        self.similar_aa = AAPool.get_similarity_groups()
        self.property_groups = AAPool.get_property_groups()

        self.site_labels = []
        self.site_types = []
        self._parse_site_data()

        self.aa_vocab = self._create_aa_vocab()
        self.aa_to_int = {aa: i + 1 for i, aa in enumerate(self.aa_vocab)}  # 0保留给填充
        self.vocab_size = len(self.aa_vocab)

        self.type_encoder = LabelEncoder()
        self._encode_site_types()
        self.num_classes = len(self.type_encoder.classes_)

        self.total_length = 2 * window_size  # 总长度应为2 * window_size
        self.samples = self.generate_samples()

        self.class_distribution = self._calculate_class_distribution()

    def _parse_site_data(self) -> None:
        for sites, types in zip(self.df['site_labels'], self.df['site_types']):
            # 处理特殊格式（替换~为"~以便ast.literal_eval解析）
            sites = re.sub(r'(\d+)\~', r'"\1~', sites)
            types = re.sub(r'(\d+)\~', r'"\1~', types)

            # 解析列表
            try:
                self.site_labels.append(ast.literal_eval(sites))
                self.site_types.append(ast.literal_eval(types))
            except:
                # 处理解析错误，假设单个值
                self.site_labels.append([int(sites)])
                self.site_types.append([types])

    def _create_aa_vocab(self) -> List[str]:

        aa_set = set()
        for seq in self.sequences:
            aa_set.update(seq)
        # 确保包含填充字符X
        if 'X' not in aa_set:
            aa_set.add('X')
        return sorted(aa_set)

    def _encode_site_types(self) -> None:

        all_types = []
        for stype_list in self.site_types:
            all_types.extend(stype_list)
        self.type_encoder.fit(all_types)

    def _calculate_class_distribution(self) -> Dict[int, int]:

        all_types = [stype for _, stype in self.samples]
        return dict(Counter(all_types))

    def augment_sequence(self, seq: str) -> str:

        if not self.augment or random.random() < 0.7:  # 30%概率不增强
            return seq

        seq_list = list(seq)
        num_mutations = random.randint(1, min(3, len(seq_list) // 10 + 1))  # 1-3个突变，或序列长度的10%

        for _ in range(num_mutations):
            pos = random.randint(0, len(seq_list) - 1)
            aa = seq_list[pos]

            if aa == 'X':
                continue  # 不突变填充字符

            if self.augment_strategy == "similarity" and aa in self.similar_aa:
                if len(self.similar_aa[aa]) > 0:
                    seq_list[pos] = random.choice(self.similar_aa[aa])
            elif self.augment_strategy == "property":
                # 找到氨基酸所属性质组
                group = None
                for g, aas in self.property_groups.items():
                    if aa in aas:
                        group = g
                        break
                if group:
                    # 从同组中随机选择一个不同的氨基酸
                    possible = [a for a in self.property_groups[group] if a != aa]
                    if possible:
                        seq_list[pos] = random.choice(possible)
            elif self.augment_strategy == "random":
                # 随机选择一个不同的氨基酸
                possible = [a for a in self.aa_vocab if a != aa]
                if possible:
                    seq_list[pos] = random.choice(possible)

        return ''.join(seq_list)

    def generate_samples(self) -> List[Tuple[List[int], int]]:

        samples = []

        for prot_id, (seq, sites, types) in enumerate(zip(self.sequences, self.site_labels, self.site_types)):
            # 截断过长序列
            seq = seq[:self.max_seq_length]

            for site, stype in zip(sites, types):
                # 处理多位置位点（取中心位置）
                if isinstance(site, list):
                    center = int(np.mean(site))
                else:
                    center = site

                center = max(1, min(center, len(seq)))  # 至少1，不超过序列长度

                start = max(0, center - 1 - self.window_size)
                end = min(len(seq), center + self.window_size)

                context = seq[start:end]

                # 应用数据增强
                context = self.augment_sequence(context)

                total_needed = self.total_length
                current_length = len(context)
                pad_needed = total_needed - current_length

                pad_left = max(0, self.window_size - (center - 1 - start))
                pad_right = max(0, pad_needed - pad_left)

                context = 'X' * pad_left + context + 'X' * pad_right
                context = context[:self.total_length]  # 确保长度一致

                encoded = [self.aa_to_int.get(aa, 0) for aa in context]

                encoded_type = self.type_encoder.transform([stype])[0]

                samples.append((encoded, encoded_type))

        return samples

    def __len__(self) -> int:

        return len(self.samples)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """获取指定索引的样本"""
        sequence, site_type = self.samples[idx]
        return (torch.tensor(sequence, dtype=torch.long),
                torch.tensor(site_type, dtype=torch.long))

    def get_class_weights(self, method: str = "inverse") -> torch.Tensor:

        class_counts = np.array([self.class_distribution.get(i, 1) for i in range(self.num_classes)])

        if method == "inverse":
            weights = 1. / class_counts
        elif method == "log":
            weights = np.log(class_counts.max() / class_counts)
        elif method == "sqrt":
            weights = np.sqrt(class_counts.max() / class_counts)
        else:
            weights = np.ones_like(class_counts)

        # 归一化
        weights = weights / weights.sum() * len(weights)
        return torch.tensor(weights, dtype=torch.float).to(device)

    def get_sequence_stats(self) -> Dict[str, float]:
        seq_lengths = [len(seq) for seq in self.sequences]
        return {
            "mean_length": np.mean(seq_lengths),
            "median_length": np.median(seq_lengths),
            "min_length": min(seq_lengths),
            "max_length": max(seq_lengths),
            "std_length": np.std(seq_lengths)
        }

    def visualize_class_distribution(self, save_path: str = None) -> None:
        """可视化类别分布"""
        classes = self.type_encoder.classes_
        counts = [self.class_distribution.get(self.type_encoder.transform([c])[0], 0) for c in classes]

        plt.figure(figsize=(10, 6))
        sns.barplot(x=classes, y=counts)
        plt.title('Class Distribution')
        plt.xlabel('Class')
        plt.ylabel('Sample Count')
        plt.xticks(rotation=45)

        if save_path:
            plt.savefig(save_path)
            print(f"Class distribution plot saved to {save_path}")

        plt.show()


# 自定义collate函数，处理变长序列
def variable_length_collate(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor]:

    sequences, labels = zip(*batch)
    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)
    max_len = max(lengths)

    padded_sequences = []
    for seq in sequences:
        pad_len = max_len - len(seq)
        padded_seq = torch.cat([seq, torch.zeros(pad_len, dtype=torch.long)])
        padded_sequences.append(padded_seq)

    return torch.stack(padded_sequences), lengths, torch.stack(labels)


# 位置编码模块
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, max_seq_length: int = 1000):

        super(PositionalEncoding, self).__init__()

        # 创建位置编码矩阵
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:

        return x + self.pe[:, :x.size(1), :]


# 改进的模型架构
class AdvancedEnzymeClassifier(nn.Module):

    def __init__(self,
                 vocab_size: int,
                 embed_dim: int = 64,
                 num_classes: int = 4,
                 window_size: int = 15,
                 use_lstm: bool = True,
                 use_cnn: bool = True,
                 use_attention: bool = True,
                 use_transformer: bool = False,
                 dropout_rate: float = 0.3):

        super(AdvancedEnzymeClassifier, self).__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.num_classes = num_classes
        self.window_size = window_size
        self.use_lstm = use_lstm
        self.use_cnn = use_cnn
        self.use_attention = use_attention
        self.use_transformer = use_transformer
        self.dropout_rate = dropout_rate

        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)

        self.positional_encoding = PositionalEncoding(embed_dim)

        self.cnn_features = None
        if use_cnn:
            self.cnn_features = self._create_cnn_layers()

        self.lstm_features = None
        if use_lstm:
            self.lstm_features = self._create_lstm_layers()

        self.attention = None
        if use_attention:
            self.attention = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)
            self.attn_norm = nn.LayerNorm(embed_dim)
            self.attn_dropout = nn.Dropout(dropout_rate)

        self.transformer_encoder = None
        if use_transformer:
            encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=2, batch_first=True)
            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)

        self.feature_fusion = self._create_feature_fusion()

        self.classifier = self._create_classifier()

        self.type_encoder = None
        self.aa_to_int = None

    def _create_cnn_layers(self) -> nn.Module:
        return nn.Sequential(
            nn.Conv1d(self.embed_dim, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(self.dropout_rate),
            nn.Conv1d(64, 128, kernel_size=5, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(self.dropout_rate),
            nn.Conv1d(128, 256, kernel_size=7, padding=3),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )

    def _create_lstm_layers(self) -> nn.Module:
        return nn.LSTM(
            input_size=self.embed_dim,
            hidden_size=128,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=self.dropout_rate if 2 > 1 else 0
        )

    def _create_feature_fusion(self) -> nn.Module:

        cnn_dim = 256 if self.use_cnn else 0
        lstm_dim = 256 if self.use_lstm else 0
        attn_dim = self.embed_dim if self.use_attention else 0
        transformer_dim = self.embed_dim if self.use_transformer else 0

        total_dim = cnn_dim + lstm_dim + attn_dim + transformer_dim

        if total_dim > 0:
            return nn.Sequential(
                nn.Linear(total_dim, 512),
                nn.BatchNorm1d(512),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate)
            )
        else:
            return nn.Identity()

    def _create_classifier(self) -> nn.Module:

        # 计算输入维度
        fusion_dim = 512 if self.feature_fusion else 0
        if fusion_dim == 0:
            fusion_dim = self.embed_dim  # 默认使用嵌入维度

        return nn.Sequential(
            nn.Linear(fusion_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(self.dropout_rate),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(self.dropout_rate),

            nn.Linear(128, self.num_classes)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:

        batch_size = x.size(0)
        seq_length = x.size(1)

        # 嵌入层
        x_embed = self.embedding(x)  # (batch, seq, embed)

        # 位置编码
        x_embed = self.positional_encoding(x_embed)

        # 特征提取分支
        features = []

        # CNN分支
        if self.use_cnn:
            x_embed_t = x_embed.transpose(1, 2)  # (batch, embed, seq)
            cnn_out = self.cnn_features(x_embed_t).squeeze(-1)  # (batch, 256)
            features.append(cnn_out)

        # LSTM分支
        if self.use_lstm:
            lstm_out, _ = self.lstm_features(x_embed)  # (batch, seq, 256)
            lstm_out = lstm_out[:, -1, :]  # 取最后一个时间步 (batch, 256)
            features.append(lstm_out)

        # 注意力分支
        if self.use_attention:
            attn_output, _ = self.attention(x_embed, x_embed, x_embed)
            attn_output = self.attn_norm(attn_output + x_embed)  # 残差连接
            attn_output = self.attn_dropout(attn_output)
            attn_output = attn_output.mean(dim=1)  # (batch, embed_dim)
            features.append(attn_output)

        # Transformer分支
        if self.use_transformer:
            transformer_output = self.transformer_encoder(x_embed)
            transformer_output = transformer_output.mean(dim=1)  # (batch, embed_dim)
            features.append(transformer_output)

        # 融合特征
        if features:
            fused_features = torch.cat(features, dim=1)
            fused_features = self.feature_fusion(fused_features)
        else:
            fused_features = x_embed.mean(dim=1)  # 无特征分支时使用平均池化

        # 分类
        output = self.classifier(fused_features)
        return output


class WarmupCosineScheduler(optim.lr_scheduler._LRScheduler):
    """带有warmup阶段的余弦学习率调度器"""

    def __init__(self,
                 optimizer: optim.Optimizer,
                 num_epochs: int,
                 warmup_epochs: int = 5,
                 min_lr_ratio: float = 0.01,
                 last_epoch: int = -1):

        self.num_epochs = num_epochs
        self.warmup_epochs = warmup_epochs
        self.min_lr_ratio = min_lr_ratio
        super(WarmupCosineScheduler, self).__init__(optimizer, last_epoch)

    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # Warmup阶段：线性增加学习率
            warmup_ratio = self.last_epoch / self.warmup_epochs
            return [base_lr * warmup_ratio for base_lr in self.base_lrs]
        else:
            # 余弦衰减阶段
            progress = (self.last_epoch - self.warmup_epochs) / (self.num_epochs - self.warmup_epochs)
            cosine_ratio = 0.5 * (1 + math.cos(math.pi * progress))
            min_lr = [base_lr * self.min_lr_ratio for base_lr in self.base_lrs]
            return [base_lr * cosine_ratio + min_lr[i] for i, base_lr in enumerate(self.base_lrs)]


# 训练函数，支持混合精度训练
def train_advanced_model(
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        scheduler: optim.lr_scheduler._LRScheduler = None,
        num_epochs: int = 50,
        patience: int = 15,
        use_amp: bool = True,
        gradient_accumulation_steps: int = 1,
        log_interval: int = 100
) -> Tuple[nn.Module, Dict[str, List[float]]]:

    best_val_acc = 0.0
    best_val_loss = float('inf')
    epochs_no_improve = 0
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    # 混合精度训练
    scaler = GradScaler() if use_amp else None

    print(f"Training started with {num_epochs} epochs, patience={patience}")
    print(f"Using {'mixed precision' if use_amp else 'float32'} training")

    for epoch in range(num_epochs):
        # 训练阶段
        model.train()
        train_loss = 0.0
        correct_train = 0
        total_train = 0
        batch_count = 0

        train_iterator = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} - Training")
        for batch_idx, (inputs, lengths, labels) in enumerate(train_iterator):
            inputs, labels = inputs.to(device), labels.to(device)

            # 混合精度训练
            with autocast(enabled=use_amp):
                outputs = model(inputs)
                loss = criterion(outputs, labels) / gradient_accumulation_steps  # 平均损失用于累积

            # 反向传播
            scaler.scale(loss).backward() if use_amp else loss.backward()

            # 梯度累积
            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
                # 梯度裁剪
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                # 更新参数
                scaler.step(optimizer) if use_amp else optimizer.step()
                scaler.update() if use_amp else None

                # 清空梯度
                optimizer.zero_grad()

            # 统计
            train_loss += loss.item() * gradient_accumulation_steps * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
            batch_count += 1

            # 打印训练日志
            if (batch_idx + 1) % log_interval == 0:
                current_lr = optimizer.param_groups[0]['lr']
                avg_loss = train_loss / (batch_count * inputs.size(0))
                train_iterator.set_postfix({
                    'loss': f"{avg_loss:.4f}",
                    'acc': f"{correct_train / total_train:.4f}",
                    'lr': f"{current_lr:.6f}"
                })

        train_loss = train_loss / len(train_loader.dataset)
        train_acc = correct_train / total_train

        model.eval()
        val_loss = 0.0
        correct_val = 0
        total_val = 0

        with torch.no_grad():
            val_iterator = tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} - Validation")
            for inputs, lengths, labels in val_iterator:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        val_loss = val_loss / len(val_loader.dataset)
        val_acc = correct_val / total_val

        if scheduler:
            scheduler.step()

        # 记录历史
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)

        # 打印 epoch 总结
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nEpoch {epoch + 1}/{num_epochs}: "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | "
              f"LR: {current_lr:.6f}")

        # 早停机制
        if val_loss < best_val_loss or val_acc > best_val_acc:
            if val_loss < best_val_loss:
                best_val_loss = val_loss
            if val_acc > best_val_acc:
                best_val_acc = val_acc
            epochs_no_improve = 0
            # 保存最佳模型
            torch.save(model.state_dict(), 'best_advanced_model.pth')
            print(f"Validation metrics improved. Saving model...")
        else:
            epochs_no_improve += 1
            print(f"No improvement in validation metrics for {epochs_no_improve} epochs.")

            if epochs_no_improve >= patience:
                print(f"Early stopping at epoch {epoch + 1}!")
                break

    # 加载最佳模型
    if os.path.exists('best_advanced_model.pth'):
        model.load_state_dict(torch.load('best_advanced_model.pth'))

    print(f"Training completed. Best validation accuracy: {best_val_acc:.4f}")
    return model, history


# 评估函数，支持详细的模型分析
# 评估函数，支持详细的模型分析
def evaluate_advanced_model(
        model: nn.Module,
        test_loader: DataLoader,
        plot_confusion: bool = True,
        plot_roc: bool = True,
        save_dir: str = "results"
) -> Tuple[float, np.ndarray]:
    """
    评估高级酶分类模型，提供详细的评估指标和可视化

    参数:
        model: 训练好的模型
        test_loader: 测试数据加载器
        plot_confusion: 是否绘制混淆矩阵
        plot_roc: 是否绘制ROC曲线
        save_dir: 结果保存目录
    """
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    model.eval()
    correct = 0
    total = 0
    all_labels = []
    all_preds = []
    all_probs = []

    with torch.no_grad():
        for inputs, lengths, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    accuracy = correct / total
    print(f"Test Accuracy: {accuracy:.4f}")

    # 计算详细指标
    if model.num_classes > 2:
        # 多分类情况
        report = classification_report(all_labels, all_preds, output_dict=True)
        cm = confusion_matrix(all_labels, all_preds)

        # 打印分类报告
        print("\nClassification Report:")
        print(classification_report(all_labels, all_preds, target_names=[str(c) for c in model.type_encoder.classes_]))  # 修改此行

        # 保存报告
        with open(os.path.join(save_dir, "classification_report.txt"), "w") as f:
            f.write(classification_report(all_labels, all_preds, target_names=[str(c) for c in model.type_encoder.classes_]))  # 修改此行

        # 绘制混淆矩阵
        if plot_confusion:
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=[str(c) for c in model.type_encoder.classes_],  # 修改此行
                        yticklabels=[str(c) for c in model.type_encoder.classes_])  # 修改此行
            plt.xlabel('Predicted')
            plt.ylabel('True')
            plt.title('Confusion Matrix')
            plt.savefig(os.path.join(save_dir, "confusion_matrix.png"))
            plt.close()

        # 绘制各类别ROC曲线（仅当类别数为2时）
        if model.num_classes == 2:
            if plot_roc:
                fpr, tpr, _ = roc_curve(all_labels, all_probs[:, 1])
                roc_auc = auc(fpr, tpr)

                plt.figure(figsize=(8, 6))
                plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
                plt.xlim([0.0, 1.0])
                plt.ylim([0.0, 1.05])
                plt.xlabel('False Positive Rate')
                plt.ylabel('True Positive Rate')
                plt.title('Receiver Operating Characteristic')
                plt.legend(loc="lower right")
                plt.savefig(os.path.join(save_dir, "roc_curve.png"))
                plt.close()
    else:
        # 二分类情况
        precision = precision_score(all_labels, all_preds)
        recall = recall_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds)

        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-score: {f1:.4f}")

        # 保存指标
        with open(os.path.join(save_dir, "binary_metrics.txt"), "w") as f:
            f.write(f"Accuracy: {accuracy:.4f}\n")
            f.write(f"Precision: {precision:.4f}\n")
            f.write(f"Recall: {recall:.4f}\n")
            f.write(f"F1-score: {f1:.4f}\n")

        # 绘制ROC曲线
        if plot_roc:
            fpr, tpr, _ = roc_curve(all_labels, all_probs[:, 1])
            roc_auc = auc(fpr, tpr)

            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Receiver Operating Characteristic')
            plt.legend(loc="lower right")
            plt.savefig(os.path.join(save_dir, "roc_curve.png"))
            plt.close()

    # 保存预测结果
    results_df = pd.DataFrame({
        'true_label': all_labels,
        'predicted_label': all_preds
    })
    for i in range(model.num_classes):
        results_df[f'prob_class_{i}'] = [p[i] for p in all_probs]

    results_df.to_csv(os.path.join(save_dir, "prediction_results.csv"), index=False)
    print(f"Prediction results saved to {os.path.join(save_dir, 'prediction_results.csv')}")

    return accuracy, cm if model.num_classes > 2 else None


# 主函数
def main_advanced():
    data_path = "dataset.csv"  # 替换为实际数据路径

    results_dir = "results_advanced"
    os.makedirs(results_dir, exist_ok=True)

    # 初始化数据集
    print("Loading dataset...")
    dataset = EnhancedEnzymeDataset(
        file_path=data_path,
        window_size=15,
        augment=True,
        augment_strategy="similarity",
        use_positional_encoding=True,
        max_seq_length=1000
    )

    stats = dataset.get_sequence_stats()
    print(f"Dataset statistics:")
    print(f"  Number of samples: {len(dataset)}")
    print(f"  Number of classes: {dataset.num_classes}")
    print(f"  Vocabulary size: {dataset.vocab_size}")
    print(f"  Sequence length (mean/median): {stats['mean_length']:.1f}/{stats['median_length']:.1f}")

    dataset.visualize_class_distribution(os.path.join(results_dir, "class_distribution.png"))

    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # 创建数据加载器
    train_loader = DataLoader(
        train_dataset,
        batch_size=64,
        shuffle=True,
        collate_fn=variable_length_collate,
        num_workers=4,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=128,
        shuffle=False,
        collate_fn=variable_length_collate,
        num_workers=4,
        pin_memory=True
    )

    class_weights = dataset.get_class_weights(method="sqrt")

    model = AdvancedEnzymeClassifier(
        vocab_size=dataset.vocab_size,
        embed_dim=64,
        num_classes=dataset.num_classes,
        window_size=15,
        use_lstm=True,
        use_cnn=True,
        use_attention=True,
        use_transformer=False,
        dropout_rate=0.3
    ).to(device)

    model.type_encoder = dataset.type_encoder
    model.aa_to_int = dataset.aa_to_int

    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)

    # 学习率调度器
    scheduler = WarmupCosineScheduler(
        optimizer,
        num_epochs=50,
        warmup_epochs=5,
        min_lr_ratio=0.01
    )

    # 训练模型
    print("Training model...")
    trained_model, history = train_advanced_model(
        model=model,
        train_loader=train_loader,
        val_loader=test_loader,  # 注意：实际应用中应使用独立验证集
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=50,
        patience=15,
        use_amp=True,
        gradient_accumulation_steps=1,
        log_interval=50
    )

    # 保存训练历史
    with open(os.path.join(results_dir, "training_history.pkl"), "wb") as f:
        pickle.dump(history, f)

    # 绘制训练历史
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training and Validation Loss')

    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Train Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Training and Validation Accuracy')

    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, "training_history.png"))
    plt.close()

    # 评估模型
    print("Evaluating model...")
    accuracy, cm = evaluate_advanced_model(
        model=trained_model,
        test_loader=test_loader,
        plot_confusion=True,
        plot_roc=True,
        save_dir=results_dir
    )

    # 保存最终模型
    torch.save({
        'model_state_dict': trained_model.state_dict(),
        'type_encoder': dataset.type_encoder,
        'aa_to_int': dataset.aa_to_int,
        'accuracy': accuracy
    }, os.path.join(results_dir, "final_model.pth"))

    print(f"Training and evaluation completed. Results saved to {results_dir}")


if __name__ == "__main__":
    main_advanced()
